{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# Mecanismos de atención\n",
        "\n",
        "En este notebook, exploraremos cómo resolver un problema de análisis de sentimientos utilizando mecanismos de **self-attention**.\n",
        "\n",
        "El mecanismo de self-attention permite que cada elemento de una secuencia preste atención a otros elementos de la misma secuencia. Esto es útil para capturar dependencias a **largo plazo** a diferencia de las redes recurrentes tradicionales.\n",
        "\n",
        "Además, a diferencia de las redes LSTM o GRU, estas son **altamente paralelizables** dado que permiten tratar cada elemento de la secuencia de forma individual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkE49jCZAk6"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "Utilizaremos un conjunto de datos pensado para la tarea de análisis de sentimientos. En este caso hemos optado por un conjunto de twits escritos en inglés, los cuales ya vienen etiquetados en tres clases:\n",
        "* Positive\n",
        "* Neutral\n",
        "* Negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_url = \"https://raw.githubusercontent.com/pablo-pnunez/datasets/master/texto/sentiment_analysis_twitter.csv\"\n",
        "twitter_data = pd.read_csv(data_url)\n",
        "tweets, tweets_sentiment = twitter_data[\"text\"].str.strip().values, twitter_data[\"sentiment\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesar texto\n",
        "\n",
        "Normalmente los conjuntos de datos contienen errores o elementos no deseados, en esta parte los eliminaremos. \n",
        "\n",
        "Algunos ejemplos típicos son los números, los saltos de línea `\\n` o los tabuladores `\\t`.\n",
        "\n",
        "En esta parte separaremos además las entradas y salidas esperadas del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import nltk\n",
        "from unidecode import unidecode\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for idx, tweet in enumerate(tweets):\n",
        "    try:\n",
        "        # Eliminar saltos de línea y retornos de carro\n",
        "        tweet = re.sub(r'[\\n\\r]+', ' ', tweet)\n",
        "        # Eliminar acentos utilizando la librería unidecode\n",
        "        tweet = unidecode(tweet)\n",
        "        # Pasar a minúsculas y eliminar resto de caracteres extraños (dos puntos, punto y coma, números...)\n",
        "        tweet = re.sub(r'[^a-z\\s]', '', tweet.lower())\n",
        "        # Eliminamos más de un espacio\n",
        "        tweet = re.sub(r'(\\s)+', ' ', tweet)\n",
        "        # Filtrar secuencias de 1 palabra o menos\n",
        "        if len(tweet.split()) > 1:\n",
        "            x_data.append(tweet)\n",
        "            y_data.append(tweets_sentiment[idx])\n",
        "    except:\n",
        "        print(f\"Error con el ejemplo {idx}, no se añade.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtener vocabulario\n",
        "\n",
        "Una vez tenemos todas las frases, para codificar cada una de las palabras del texto es necesario obtener el llamado `corpus` o `vocabulario`. Este nos sirve para saber que palabras únicas (sin repetición) aparecen en nuestro texto, lo cual es útil para asignar un índice a cada una. Como verás, en este caso asignamos el índice en función de la frecuencia de aparición, por tanto, la palabra más frecuente tendrá el índice más bajo.\n",
        "\n",
        "Como recordarás, nuestro objetivo es que, dado un texto de máximo 5 palabras, el modelo entrenado sea capaz de continuar la frase hasta que decida finalizarla. Para lograrlo tenemos que tener en cuenta varios escenarios:\n",
        "* ¿Y si nos dan menos de 5 palabras?\n",
        "* ¿Como hacemos para detectar que la palabra generada por el modelo es la última de la frase? ¿Y la primera?\n",
        "* ¿Y si nos dan palabras fuera del vocabulario?\n",
        "\n",
        "Para solventar estos problemas, vamos a añadir dos palabras nuevas o `tokens` nuevos a nuestro vocabulario:\n",
        "* **\\<pad\\>**: Token que representa *padding*, es decir, si nos dan menos de 5 palabras, rellenaremos las que falten con este padding.\n",
        "* **\\<unk\\>**: Token que representa *unknown*, es decir, palabra desconocida (fuera del vocabulario).\n",
        "* **\\<eos\\>**: Token que representa *end-of-sequence*, es decir, fin de secuencia. Si el modelo predice este token, sabremos que ya se acabó la frase y podemos detener la generación.\n",
        "* **\\<sos\\>**: Token que representa *start-of-sequence*, es decir, comienzo de secuencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Crear una lista con todas las palabras del texto\n",
        "words = \" \".join(x_data).split()\n",
        "# Obtener el corpus del texto (todas las palabras que aparecen y su frecuencia)\n",
        "word_counts = Counter(words)\n",
        "# Filtrar palabras que aparecen menos de 5 veces, dado que no aportan mucho\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >=5}\n",
        "# Ordenamos el corpus por frecuencia\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "# Extraemos solo las claves del diccionario anterior para crear el corpus\n",
        "vocab = list(word_counts.keys())\n",
        "# Añadimos tokens especiales <pad> (padding) y <unk> (unknown)\n",
        "vocab = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"] + vocab\n",
        "# Generamos un diccionario (y su inverso) donde asignamos un id a cada palabra según su frecuencia.\n",
        "# La palabra más frecuente será la 1. El 0 lo dejamos para el padding\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Tamaño del vocabulario: {vocab_size}')\n",
        "print(f'Word to Index mapping: {word_to_ix}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear dataset\n",
        "Una vez tenemos nuestro conjunto limpio y analizado, podemos crear los ejemplos con los que alimentaremos nuestro modelo. \n",
        "\n",
        "En este caso simplemente tendremos que buscar la frase de mayor longitud y crear todos los vectores de ese tamaño rellenando con padding.\n",
        "\n",
        "Tendremos que crear ademas las máscaras y los vectores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# Generar dataset completo\n",
        "def generate_dataset(sentences, labels, word_to_ix, seq_length, y_map):\n",
        "    # Para cada frase, generar los ejemplos\n",
        "    x_data = []\n",
        "    mask_data = []\n",
        "    y_data = []\n",
        "\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        unk_index = word_to_ix[\"<unk>\"]\n",
        "        words = sentence.split()\n",
        "\n",
        "        # Crear la secuencia de entrada\n",
        "        seq_in = ['<pad>'] * (seq_length - len(words)) + words\n",
        "        seq_in_indices = [word_to_ix.get(word, unk_index) for word in seq_in]\n",
        "        x_data.append(seq_in_indices)\n",
        "\n",
        "        # Creamos la máscara de padding\n",
        "        mask = [1 if word != '<pad>' else 0 for word in seq_in]\n",
        "        mask_data.append(mask)\n",
        "        y_data.append(y_map[label])\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(((x_data, mask_data), y_data))\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba (80%,10%,10%)\n",
        "sentences_train, sentences_test, labels_train, labels_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "sentences_val, sentences_test, labels_val, labels_test = train_test_split(sentences_test, labels_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Definir parámetros y datos\n",
        "max_seq_length = max([len(s.split(\" \")) for s in x_data])\n",
        "y_map = {\"positive\":0, \"neutral\":1, \"negative\":2}\n",
        "batch_size = 1024\n",
        "\n",
        "# Generar dataset\n",
        "train_dataset = generate_dataset(sentences_train, labels_train, word_to_ix, max_seq_length, y_map).batch(batch_size)\n",
        "val_dataset = generate_dataset(sentences_val, labels_val, word_to_ix, max_seq_length, y_map).batch(batch_size)\n",
        "test_dataset = generate_dataset(sentences_test, labels_test, word_to_ix, max_seq_length, y_map).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfm07dlakr0"
      },
      "source": [
        "## 4.2.3. - Modelo\n",
        "Para este problema crearemos el modelo más sencillo posible:\n",
        "* **Capa Embedding**: Para cada elemento de la entrada, aprenderá un embedding que lo representará en un espacio de dimensión `embed_size`.\n",
        "* **Capa MultiheadAttention**: Esta capa permite al modelo enfocarse en diferentes partes de la secuencia de entrada simultáneamente. Cada \"cabeza\" en la atención múltiple (multi-head) puede capturar distintos patrones de relación entre los elementos de la secuencia.\n",
        "    * En la capa LSTM de la práctica anterior, el último elemento de la secuencia de salida contiene información de toda la secuencia, **en este caso no**.\n",
        "    * El número de heads se configura mediante `num_heads`.\n",
        "* **Capa Linear**: Proyecta el vector del último elemento de la salida de la LSTM en un espacio de tamaño `output_size`. En este caso `output_size` es igual al tamaño del vocabulario dado que estamos intentando resolver un problema de multi-clasificación.\n",
        "\n",
        "Nótese que no aplicamos la función `softmax` dado que esta se aplica internamente en la `CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLfuYHTqYyY-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# Hiperparámetros del modelo\n",
        "embed_size = 32\n",
        "num_heads = 1  # Número de cabezas de atención\n",
        "hidden_size = 16\n",
        "output_size = 3\n",
        "\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Definir el modelo usando la API funcional\n",
        "input_seq = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_seq')\n",
        "input_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "\n",
        "# Esta parte adapta la máscara a lo requerido por la capa attention\n",
        "expanded_mask = layers.Lambda(lambda x: tf.repeat(tf.expand_dims(x, axis=1), max_seq_length, axis=1))(input_mask)\n",
        "\n",
        "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=max_seq_length)(input_seq)\n",
        "dropout_1 = layers.Dropout(.5)(embedding_layer)\n",
        "attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size)(dropout_1, dropout_1, attention_mask=expanded_mask)\n",
        "dropout_2 = layers.Dropout(.5)(attention_output)\n",
        "flatten_output = layers.GlobalAveragePooling1D()(dropout_2)\n",
        "output_layer = layers.Dense(output_size, activation=\"softmax\")(flatten_output)\n",
        "model = Model(inputs=[input_seq, input_mask], outputs=output_layer)\n",
        "\n",
        "# Compilar el modelo (necesario en TensorFlow/Keras)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Mostrar resumen del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3al3i38a-fG"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCHWTAcbIsK",
        "outputId": "d6e59635-3403-4687-86fe-624d0f96a703"
      },
      "outputs": [],
      "source": [
        "# Definir la función de predicción\n",
        "def predict(model, text, word_to_ix, ix_to_word, seq_length, y_map):\n",
        "    # Preprocesar el texto de entrada\n",
        "    text = unidecode(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "\n",
        "    # Convertir palabras a índices\n",
        "    input_sequence = [word_to_ix.get(word, word_to_ix['<pad>']) for word in words]\n",
        "    input_sequence = input_sequence[-seq_length:]  # Tomar solo las últimas seq_length palabras\n",
        "    input_sequence = [word_to_ix['<pad>']] * (seq_length - len(input_sequence)) + input_sequence\n",
        "\n",
        "\n",
        "    # Convertir la secuencia y la máscara a tensores\n",
        "    input_tensor = tf.constant([input_sequence], dtype=tf.int64)\n",
        "    mask_tensor = tf.cast(input_tensor!=0, tf.int32)\n",
        "\n",
        "\n",
        "    output = model([input_tensor, mask_tensor], training=False)\n",
        "    \n",
        "    max_output = output.numpy().argmax()\n",
        "    max_pred = output.numpy().max()\n",
        "\n",
        "    print(f\"The text '{text}' is {list(y_map.keys())[max_output]} [{max_pred*100:0.0f}%]\")\n",
        "\n",
        "# Ejemplo de texto de entrada\n",
        "input_text = \"This product is lovely\"\n",
        "predicted_text = predict(model, input_text, word_to_ix, ix_to_word, max_seq_length, y_map)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
