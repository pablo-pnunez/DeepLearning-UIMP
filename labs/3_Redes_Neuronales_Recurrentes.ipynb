{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# Redes Neuronales Recurrentes\n",
        "\n",
        "En este notebook, exploraremos uno de los tipos de Redes Neuronales Recurrentes (RNN) más utilizados: las Long Short-Term Memory (LSTM).\n",
        "Estas están diseñadas para manejar **secuencias de datos** y resolver el problema del **desvanecimiento del gradiente** de las redes recurrentes básicas.\n",
        "\n",
        "Como ya hemos visto en teoría, en una LSTM propagamos entre cada etapa dos vectores diferentes, el de estado y la salida en si. Para obtener dichos vectores, necesitamos calcular el valor de la puerta de actualización, la de olvido y la de salida.\n",
        "\n",
        "Este proceso será transparente a nosotros dado que Keras ya nos proporciona una capa [`LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/) y simplemente tendremos que preocuparnos de manejar la secuencia de entrada y la de salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkE49jCZAk6"
      },
      "source": [
        "## Conjunto de datos\n",
        "Puesto que ahora trabajaremos con secuencias, para este ejemplo vamos a utilizar un conjunto de datos de texto. En este caso hemos optado por descargar el libro \"Trafalgar\" de Benito Pérez Galdós.\n",
        "\n",
        "Como hemos explicado en teoría, existen muchos tipos de problemas que podemos resolver con secuencias, pero en este caso nos centraremos en una de las más sencillas de implementar: predecir la siguiente palabra en una secuencia de 5 palabras dadas.\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmObRrKcih9X"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Descargar el texto de \"Trafalgar\" de Benito Pérez Galdós\n",
        "url = \"https://www.gutenberg.org/cache/epub/16961/pg16961.txt\"\n",
        "response = requests.get(url, timeout=30)\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEm5_7Ggih9Z"
      },
      "source": [
        "### Preprocesar texto\n",
        "\n",
        "Normalmente los conjuntos de datos contienen errores o elementos no deseados, en esta parte los eliminaremos. Algunos ejemplos típicos son los números, los saltos de línea `\\n` o los tabuladores `\\t`. <br>\n",
        "Si abres la url del libro en tu navegador verás que al inicio y al final nos aparece un aviso de copyright que tendremos que eliminar.\n",
        "\n",
        "En este punto verás que también separamos el texto en frases haciendo uso de la librería de texto `nltk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj7-pQgKjHnU"
      },
      "outputs": [],
      "source": [
        "! pip install unidecode\n",
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27eKxuMJih9Z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Eliminar saltos de línea y retornos de carro\n",
        "text = re.sub(r'[\\n\\r]+', ' ', text)\n",
        "# Eliminamos cabecera y fin en inglés\n",
        "text = text.split(\"FIN DE TRAFALGAR\")[0]\n",
        "text = text.split(\" -I- \")[1]\n",
        "# Dividir el texto en frases utilizando la librería nltk\n",
        "sentences = sent_tokenize(text)\n",
        "# Eliminar acentos utilizando la librería unidecode\n",
        "sentences = [unidecode(s) for s in sentences]\n",
        "# Pasar a minúsculas y eliminar resto de caracteres extraños (dos puntos, punto y coma, números...)\n",
        "sentences = [re.sub(r'[^a-z\\s]', '', s.lower()) for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAH1aBFpih9Z"
      },
      "source": [
        "### Obtener vocabulario\n",
        "\n",
        "Una vez tenemos todas las frases, para codificar cada una de las palabras del texto es necesario obtener el llamado `corpus` o `vocabulario`. Este nos sirve para saber que palabras únicas (sin repetición) aparecen en nuestro texto, lo cual es útil para asignar un índice a cada una. Como verás, en este caso asignamos el índice en función de la frecuencia de aparición, por tanto, la palabra más frecuente tendrá el índice más bajo.\n",
        "\n",
        "Como recordarás, nuestro objetivo es que, dado un texto de máximo 5 palabras, el modelo entrenado sea capaz de continuar la frase hasta que decida finalizarla. Para lograrlo tenemos que tener en cuenta varios escenarios:\n",
        "* ¿Y si nos dan menos de 5 palabras?\n",
        "* ¿Como hacemos para detectar que la palabra generada por el modelo es la última de la frase?\n",
        "* ¿Si nos dan una palabra que no existe en nuestro vocabulario? Una en otro idioma, por ejemplo.\n",
        "\n",
        "Para solventar estos problemas, vamos a añadir tres palabras nuevas o `tokens` a nuestro vocabulario:\n",
        "* **\\<pad\\>**: Token que representa *padding*, es decir, si nos dan menos de 5 palabras, rellenaremos las que falten con este padding.\n",
        "* **\\<eos\\>**: Token que representa *end-of-sequence*, es decir, fin de secuencia. Si el modelo predice este token, sabremos que ya se acabó la frase y podemos detener la generación.\n",
        "* **\\<unk\\>**: Token que representa *unknown*, es decir, desconocido. Si una palabra no aparece en el vocabulario, el modelo la codificará con este token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so68tqN5ih9a"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Crear una lista con todas las palabras del texto\n",
        "words = \" \".join(sentences).split()\n",
        "# Obtener el corpus del texto (todas las palabras que aparecen y su frecuencia)\n",
        "word_counts = Counter(words)\n",
        "# Filtrar palabras que aparecen menos de 2 veces, dado que no aportan mucho\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >=5}\n",
        "# Ordenamos el corpus por frecuencia\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "# Extraemos solo las claves del diccionario anterior para crear el corpus\n",
        "vocab = list(word_counts.keys())\n",
        "# Añadimos tokens especiales <pad> (padding), <eos> (end of sequence), y <unk> (unknown)\n",
        "vocab = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"] + vocab\n",
        "# Generamos un diccionario (y su inverso) donde asignamos un id a cada palabra según su frecuencia.\n",
        "# La palabra más frecuente será la 1. El 0 lo dejamos para el padding\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Tamaño del vocabulario: {vocab_size}')\n",
        "print(f'Word to Index mapping: {word_to_ix}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnw058hQih9a"
      },
      "source": [
        "### Análisis de datos\n",
        "En todos los problemas de aprendizaje, es muy importante hacer un análisis de los datos con los que vamos a trabajar, lo cual nos puede servir para diagnosticar o evitar futuros problemas.<br>\n",
        "En este caso realizamos un simple análisis de la frecuencia de las palabras.\n",
        "\n",
        "Como siempre suele suceder en estos casos, las palabras más comunes son las llamadas `stop-words`. Estas palabras son las que más solemos repetir en nuestros textos y las forman los artículos, las preposiciones, las conjunciones, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37_onY0Vih9a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualización de las 20 palabras más comunes\n",
        "words_x = list(word_counts.keys())[:20]\n",
        "counts_y = list(word_counts.values())[:20]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words_x, counts_y)\n",
        "plt.title('Palabras más comunes')\n",
        "plt.xlabel('Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNW3lDr9ih9b"
      },
      "source": [
        "### Crear dataset\n",
        "Una vez tenemos nuestro conjunto limpio y analizado, podemos crear los ejemplos con los que alimentaremos nuestro modelo. Como habíamos comentado queremos intentar completar una frase dadas, como máximo 5 palabras.<br>\n",
        "\n",
        "En el siguiente código crearemos los ejemplos a partir de las frases del texto descargado. Estos ejemplos han de reflejar todos los escenarios a los que luego queremos que nuestro modelo pueda enfrentarse.\n",
        "\n",
        "Por tanto, para cada frase, nuestro `Dataset` creará los siguientes ejemplos:\n",
        "\n",
        "* Frase: *Frase de ejemplo*\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>, \\<pad\\>, \\<sos\\>, frase], de\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>, \\<sos\\>, frase, de], ejemplo\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<sos\\>, frase, de, ejemplo], \\<eos\\>\n",
        "\n",
        "El primer vector es la secuencia de entrada y el elemento del final es la salida deseada. Ten en cuenta que al modelo no le damos el texto, le daremos los índices de cada palabra.\n",
        "\n",
        "Además, para manejar los tokens de padding correctamente, se generará una máscara binaria correspondiente que indica cuáles son tokens válidos y cuáles son de padding.\n",
        "\n",
        "Si nuestro `word_to_ix` fuese este:\n",
        "```python\n",
        "word_to_ix = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"frase\": 3, \"de\": 4, \"ejemplo\": 5}\n",
        "```\n",
        "\n",
        "Nuestros ejemplos quedarían así:\n",
        "* [0, 0, 0, 0, 1, 3], 4\n",
        "* [0, 0, 0, 1, 3, 4], 5\n",
        "* [0, 0, 1, 3, 4, 5], 2\n",
        "\n",
        "Y las máscaras correspondientes serían:\n",
        "\n",
        "* [0, 0, 0, 0, 0, 1]\n",
        "* [0, 0, 0, 0, 1, 1]\n",
        "* [0, 0, 0, 1, 1, 1]\n",
        "\n",
        "Estas máscaras aseguran que el modelo ignore los tokens de padding al procesar las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UynhZxJijjyt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Función para convertir una oración en una secuencia de índices\n",
        "def sentence_to_indices(sentence, word_to_ix, seq_length):\n",
        "    unk_index = word_to_ix[\"<unk>\"]\n",
        "    words = ['<sos>'] + sentence.split() + ['<eos>']\n",
        "\n",
        "    input_data = []\n",
        "    output_data = []\n",
        "    mask_data = []\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        seq_in = words[max(0, i + 1 - seq_length):i + 1]\n",
        "        seq_in = ['<pad>'] * (seq_length - len(seq_in)) + seq_in\n",
        "        seq_out = words[i + 1]\n",
        "\n",
        "        seq_in_indices = [word_to_ix.get(word, unk_index) for word in seq_in]\n",
        "        seq_out_index = word_to_ix.get(seq_out, unk_index)\n",
        "\n",
        "        mask = [1 if word != '<pad>' else 0 for word in seq_in]\n",
        "\n",
        "        if seq_out_index != unk_index:\n",
        "            input_data.append(seq_in_indices)\n",
        "            output_data.append(seq_out_index)\n",
        "            mask_data.append(mask)\n",
        "\n",
        "    return input_data, mask_data, output_data\n",
        "\n",
        "# Generar dataset completo\n",
        "def generate_dataset(sentences, word_to_ix, seq_length):\n",
        "    # Para cada frase, generar los ejemplos\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "    mask_data = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        input_data, mask, output_data = sentence_to_indices(sentence, word_to_ix, seq_length)\n",
        "        x_data.extend(input_data)\n",
        "        y_data.extend(output_data)\n",
        "        mask_data.extend(mask)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(((x_data, mask_data), y_data))\n",
        "\n",
        "# Dividimos el conjunto en entrenamiento, validación y test (80%,10%,10%) de forma aleatoria.\n",
        "sentences_train, sentences_test  = train_test_split(sentences, test_size=.2)\n",
        "sentences_val, sentences_test = train_test_split(sentences_test, test_size=.5)\n",
        "\n",
        "# Definir parámetros y datos\n",
        "seq_length = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Generar dataset\n",
        "train_dataset = generate_dataset(sentences_train, word_to_ix, seq_length).batch(batch_size)\n",
        "val_dataset = generate_dataset(sentences_val, word_to_ix, seq_length).batch(batch_size)\n",
        "test_dataset = generate_dataset(sentences_test, word_to_ix, seq_length).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfm07dlakr0"
      },
      "source": [
        "## Modelo\n",
        "Para este problema crearemos el modelo más sencillo posible:\n",
        "* **Capa Embedding**: Para cada elemento de la entrada, aprenderá un embedding que lo representará en un espacio de dimensión `embed_size`.\n",
        "* **Capa LSTM**: Procesa el embedding de cada elemento de la secuencia de entrada, lo proyecta en otro espacio de tamaño `hidden_size` y retorna la secuencia de salida.\n",
        "    * Ojo, la capa LSTM retorna, por defecto, la salida en el último instante de tiempo. Si quieres concatenar dos LSTM tendrás que indicar en la primera `return_sequences=True`.\n",
        "    * Como ya hemos comentado en teoría, en estas redes se acumula toda la información de la secuencia en ese último elemento.\n",
        "    * La máscara se pasa a la LSTM con el fin de que procese solo los elementos relevantes de la secuencia (no el padding).\n",
        "* **Capa Linear**: Proyecta el vector del último elemento de la salida de la LSTM en un espacio de tamaño `output_size`. En este caso `output_size` es igual al tamaño del vocabulario dado que estamos intentando resolver un problema de multi-clasificación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBPGaUR2ih9c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# Hiperparámetros del modelo\n",
        "embed_size = 64\n",
        "hidden_size = 32\n",
        "output_size = vocab_size\n",
        "num_epochs = 50\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Definir el modelo usando la API funcional\n",
        "input_seq = Input(shape=(seq_length,), dtype=tf.int32, name='input_seq')\n",
        "input_mask = Input(shape=(seq_length,), dtype=tf.int32, name='input_mask')\n",
        "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length)(input_seq)\n",
        "lstm_layer = layers.LSTM(hidden_size, activation=\"relu\")(embedding_layer, mask=input_mask)\n",
        "output_layer = layers.Dense(output_size, activation=\"softmax\")(lstm_layer)\n",
        "model = Model(inputs=[input_seq, input_mask], outputs=output_layer)\n",
        "\n",
        "# Compilar el modelo (necesario en TensorFlow/Keras)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Mostrar resumen del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3al3i38a-fG"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-WxrQ55oNnA"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii2cwbSPih9d"
      },
      "source": [
        "## Prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JHbsdXZxDRq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "import tensorflow as tf\n",
        "\n",
        "# Definir la función de predicción\n",
        "def predict(model, text, word_to_ix, ix_to_word, seq_length, max_len=20):\n",
        "    # Preprocesar el texto de entrada\n",
        "    text = unidecode(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = [\"<sos>\"] + text.split()\n",
        "\n",
        "    # Convertir palabras a índices\n",
        "    input_sequence = [word_to_ix.get(word, word_to_ix['<pad>']) for word in words]\n",
        "    input_sequence = input_sequence[-seq_length:]  # Tomar solo las últimas seq_length palabras\n",
        "    input_sequence = [word_to_ix['<pad>']] * (seq_length - len(input_sequence)) + input_sequence\n",
        "\n",
        "    # Crear la máscara para la secuencia de entrada\n",
        "    input_mask = [1 if word != word_to_ix['<pad>'] else 0 for word in input_sequence]\n",
        "\n",
        "    # Convertir la secuencia y la máscara a tensores\n",
        "    input_tensor = tf.constant([input_sequence], dtype=tf.int64)\n",
        "    mask_tensor = tf.constant([input_mask], dtype=tf.int64)\n",
        "\n",
        "    # Inicializar la lista de palabras generadas\n",
        "    generated_words = words[1:]\n",
        "    predicted_word = \"\"\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Hacer una predicción usando el modelo\n",
        "        output = model([input_tensor, mask_tensor], training=False)\n",
        "\n",
        "        # Obtener la palabra con la probabilidad más alta\n",
        "        predicted_index = tf.argmax(output, axis=-1).numpy()[0]\n",
        "        predicted_word = ix_to_word[predicted_index]\n",
        "\n",
        "        if predicted_word == '<eos>':\n",
        "            break\n",
        "\n",
        "        # Añadir la palabra generada a la lista\n",
        "        generated_words.append(predicted_word)\n",
        "\n",
        "        # Actualizar la secuencia de entrada y la máscara para la siguiente predicción\n",
        "        input_sequence = input_sequence[1:] + [predicted_index]\n",
        "        input_mask = input_mask[1:] + [1]\n",
        "\n",
        "        input_tensor = tf.constant([input_sequence], dtype=tf.int64)\n",
        "        mask_tensor = tf.constant([input_mask], dtype=tf.int64)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "# Ejemplo de texto de entrada\n",
        "input_text = \"la batalla de\"\n",
        "predicted_text = predict(model, input_text, word_to_ix, ix_to_word, seq_length)\n",
        "print(predicted_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
