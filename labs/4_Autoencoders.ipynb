{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En las próximas prácticas vamos a crear arquitecturas propias del aprendizaje no supervisado. Como ya sabrás, este tipo de problemas se caracterizan por la falta de etiquetas en los datos, lo que significa que no contamos con una respuesta correcta para cada entrada.\n",
        "\n",
        "En su lugar, buscamos descubrir patrones, estructuras y relaciones dentro de los datos. Una de las redes neuronales profundas más utilizadas para este tipo de problemas son los **Autoencoders**.\n",
        "\n",
        "Un Autoencoder es un tipo de red neuronal utilizada para aprender representaciones eficientes (codificaciones) de los datos, típicamente para la reducción de dimensionalidad.\n",
        "\n",
        "Durante su entrenamiento tienen como objetivo reconstruir su entrada en la salida, pasando la información a través de una red de capas más pequeñas.\n",
        "\n",
        "### Estructura de un Autoencoder\n",
        "\n",
        "![image](https://i.imgur.com/0gIK8Gd.png)\n",
        "\n",
        "Estos constan de dos partes principales:\n",
        "\n",
        "1. **Codificador (Encoder)**: Comprime la entrada a un espacio *latente* de menor dimensión que el espacio original.\n",
        "2. **Decodificador (Decoder)**: Reconstruye los datos originales a partir de la representación compacta creada por el codificador.\n",
        "\n",
        "En esta práctica entrenaremos un Autoencoder convolucional capaz de aprender una codificación en un espacio de 2D ($\\mathbb{R}^{2}$) para cada una de las imágenes de un conjunto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "En este caso hemos optado por utilizar el conjunto [MNIST](https://es.wikipedia.org/wiki/Base_de_datos_MNIST). Este conjunto posee imágenes en escala de grises con números del 0 al 9 escritos a mano por personas. Cada una de estas imágenes tiene una resolución de $28 \\times 28$ píxels. Al ser en escala de grises, cada imagen será un tensor de $1 \\times 28 \\times 28$.\n",
        "\n",
        "Este conjunto de datos está pensado para resolver un problema de *Aprendizaje supervisado de multiclasificación*, pero en este caso descartaremos las etiquetas y utilizaremos solamente las imágenes.\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos el módulo `datasets` de `keras` para obtener el conjunto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Fijar la semilla para obtener reproducibilidad\n",
        "seed = 42\n",
        "keras.utils.set_random_seed(seed)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como puedes ver a continuación, este conjunto está formado por 60000 imágenes de train y 10000 de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las imágenes vienen sin normalizar, por tanto lo primero será pasarlas del rango $[0,255]$ al rango $[0,1]$ y luego crearemos los datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizar los datos al rango [0, 1]\n",
        "x_train, x_test = x_train /255, x_test / 255\n",
        "\n",
        "# Crear los datasets de tensorflow\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, x_train))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoencoder Convolucional\n",
        "\n",
        "Los autoencoders no poseen una arquitectura definida; esta depende del problema y del tipo de datos con los que trabajemos. Lo único que definen es la necesidad de crear una parte *encoder* y otra *decoder*.\n",
        "\n",
        "Al trabajar con imágenes, nuestro encoder estará formado por capas convolucionales que procesarán la imagen y la proyectarán en un espacio de, en este caso, $\\mathbb{R}^{2}$. \n",
        "\n",
        "El decoder hará el proceso inverso: deberá generar, dado un vector 2D, una salida de $1 \\times 28 \\times 28$. Para llevar a cabo este proceso serán necesarias capas denominadas *deconvoluciones* o *convoluciones transpuestas*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_autoencoder(k=2, learning_rate=1e-4):\n",
        "\n",
        "    # Definimos el encoder\n",
        "    input_img = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(8, (3, 3), strides=2, padding='same', activation='tanh')(input_img)  # (batch, 14, 14, 8)\n",
        "    x = layers.Conv2D(16, (3, 3), strides=2, padding='same', activation='tanh')(x)  # (batch, 7, 7, 16)\n",
        "    x = layers.Conv2D(32, (3, 3), strides=2, padding='same', activation='tanh')(x)  # (batch, 4, 4, 32)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation='tanh')(x)  # Proyección a vector de 64D\n",
        "    encoded = layers.Dense(k)(x)  # Proyección a vector de K\n",
        "\n",
        "    encoder = models.Model(input_img, encoded, name='encoder')\n",
        "\n",
        "    # Definimos el decoder\n",
        "    encoded_input = layers.Input(shape=(k,))\n",
        "    x = layers.Dense(64, activation='tanh')(encoded_input)\n",
        "    x = layers.Dense(16 * 7 * 7, activation='tanh')(x)\n",
        "    x = layers.Reshape((7, 7, 16))(x)\n",
        "    x = layers.Conv2DTranspose(8, (3, 3), strides=2, padding='same', activation='tanh')(x)  # (batch, 16, 16, 8)\n",
        "    decoded = layers.Conv2DTranspose(1, (3, 3), strides=2, padding='same', activation='sigmoid', output_padding=1)(x)  # (batch, 28, 28, 1)\n",
        "\n",
        "    decoder = models.Model(encoded_input, decoded, name='decoder')\n",
        "    \n",
        "    # Conectar el encoder y el decoder para crear el autoencoder completo\n",
        "    autoencoder_input = layers.Input(shape=(28, 28, 1))\n",
        "    encoded_output = encoder(autoencoder_input)\n",
        "    decoded_output = decoder(encoded_output)\n",
        "\n",
        "    autoencoder = models.Model(autoencoder_input, decoded_output, name='autoencoder')\n",
        "    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "    \n",
        "    return encoder, decoder, autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear autoencoder que proyecta en k=2\n",
        "_, _, autoencoder = create_autoencoder()\n",
        "\n",
        "# Resumen del modelo autoencoder\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callback\n",
        "A continuación vamos a crear un `callback` para visualizar, tras cada epoch, las imagenes reales y las predichas por el autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Borrar la consola\n",
        "        clear_output(wait=True)\n",
        "        # Seleccionar un lote del conjunto de prueba\n",
        "        for test_batch in self.test_data.take(1):\n",
        "            test_images, _ = test_batch\n",
        "\n",
        "            # Generar imágenes utilizando el autoencoder\n",
        "            generated_images = self.model(test_images)\n",
        "            \n",
        "            # Mostrar las imágenes originales y generadas\n",
        "            fig, axs = plt.subplots(2, 10, figsize=(10, 2))\n",
        "            for i in range(10):\n",
        "                # Mostrar la imagen original\n",
        "                axs[0, i].imshow(test_images[i], cmap='gray')\n",
        "                axs[0, i].axis('off')\n",
        "                \n",
        "                # Mostrar la imagen generada\n",
        "                axs[1, i].imshow(generated_images[i], cmap='gray')\n",
        "                axs[1, i].axis('off')\n",
        "            \n",
        "            plt.suptitle(f'Epoch {epoch + 1}')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar autoencoder con $k=2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de parámetros\n",
        "batch_size = 64\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 25\n",
        "\n",
        "# Crear el autoencoder\n",
        "encoder, decoder, autoencoder = create_autoencoder(k=2, learning_rate=learning_rate)\n",
        "\n",
        "# Definir los batches\n",
        "train_dataset = train_data.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_data.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Inicializamos el callcback previo\n",
        "callback = DisplayCallback(test_dataset)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history = autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=num_epochs, verbose=1, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al reducir a un espacio de 2 dimensiones, podemos visualizar cada imagen como un punto en un gráfico de dispersión coloreando cada uno según el dígito que representa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "image_embs = encoder.predict(test_dataset)\n",
        "\n",
        "# Crear el scatterplot\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(image_embs[:, 0], image_embs[:, 1], c=y_test, alpha=0.7)\n",
        "plt.title('Scatterplot de las características codificadas')\n",
        "plt.xlabel('Componente 1')\n",
        "plt.ylabel('Componente 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar autoencoder con $k=8$\n",
        "Como se ha visto, reducir a un espacio de $k=2$ permite reconstruir las imágenes originales, pero a costa de perder cierta calidad. Probaremos a continuación la reducción a espacios de mayor dimensión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conectar el encoder y el decoder para crear el autoencoder completo\n",
        "encoder, decoder, autoencoder = create_autoencoder(k=8, learning_rate=learning_rate)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history = autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=num_epochs, verbose=1, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar autoencoder con $k=16$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conectar el encoder y el decoder para crear el autoencoder completo\n",
        "encoder, decoder, autoencoder = create_autoencoder(k=16, learning_rate=learning_rate)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history = autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=num_epochs, verbose=1, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolación\n",
        "Para finalizar vamos a utilizar el decoder para generar nuevas imágenes por interpolación. Básicamente tendremos que obtener el vector de dos imágenes reales utilizando el encoder y luego generar $n$ vectores intermedios y ver que imágenes resultan utilizando el decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = x_test[-1] # Una imagen de un 6\n",
        "end = x_test[3] # Una imagen de un 0\n",
        "\n",
        "# Obtener su proyección en el espacio k\n",
        "start_emb = encoder(tf.expand_dims(start, 0))\n",
        "end_emb = encoder(tf.expand_dims(end, 0))\n",
        "\n",
        "# Número de puntos total incluidos extremos\n",
        "x = 10\n",
        "# Generar puntos equidistantes entre start_emb y end_emb\n",
        "interpolated_embs = tf.squeeze(tf.linspace(start_emb, end_emb, num=x))\n",
        "# Obtener imágenes resultantes\n",
        "images = decoder(interpolated_embs)\n",
        "\n",
        "plt.figure(figsize=(x*1.5, 4))\n",
        "\n",
        "for idx, im in enumerate(images): \n",
        "    # Imagen original\n",
        "    plt.subplot(1, len(images), idx+1)\n",
        "    plt.imshow(im, cmap='gray')\n",
        "    plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
